# pyright: reportMissingImports=false
from airflow.operators.dummy_operator import DummyOperator
from airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator
from airflow.models import Variable
from airflow_dbt.operators.dbt_operator import DbtRunOperator, DbtSeedOperator, DbtSnapshotOperator
from airflow.hooks.base_hook import BaseHook
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.decorators import dag
from datetime import datetime, timedelta
import pytz

################################# VARIABLES #################################
connection_id = Variable.get('airbyte_connection_id')
SLACK_CONN_ID = Variable.get('slack')


#### Helper Functions ####
def convert_datetime(datetime_string):
    return datetime_string.astimezone(pytz.timezone('UTC')).strftime('%b-%d %H:%M:%S')


##### Slack Alerts #####
def task_fail_slack_alert(context):
    # Called on failure
    slack_webhook_token = BaseHook.get_connection(SLACK_CONN_ID).password
    channel = BaseHook.get_connection(SLACK_CONN_ID).login
    slack_msg = f"""
        :x: Task Failed.
        *Task*: {context.get('task_instance').task_id}
        *Dag*: {context.get('task_instance').dag_id}
        *Execution Time*: {convert_datetime(context.get('execution_date'))}
        <{context.get('task_instance').log_url}|*Logs*>
    """

    slack_alert = SlackWebhookOperator(
        task_id='slack_fail',
        webhook_token=slack_webhook_token,
        message=slack_msg,
        channel=channel,
        username='airflow',
        http_conn_id=SLACK_CONN_ID
    )

    return slack_alert.execute(context=context)


def task_succeed_slack_alert(context):
    # Called on success
    slack_webhook_token = BaseHook.get_connection(SLACK_CONN_ID).password
    channel = BaseHook.get_connection(SLACK_CONN_ID).login
    slack_msg = f"""
        :white_check_mark: Task Succeeded!
        *Dag*: {context.get('task_instance').dag_id}
        *Execution Time*: {convert_datetime(context.get('execution_date'))}
        <{context.get('task_instance').log_url}|*Logs*>
    """

    slack_alert = SlackWebhookOperator(
        task_id='slack_success',
        webhook_token=slack_webhook_token,
        message=slack_msg,
        channel=channel,
        username='airflow',
        http_conn_id=SLACK_CONN_ID
    )

    return slack_alert.execute(context=context)


# Default args for Airflow DAGs
default_args = {
    "owner": "airflow",
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "start_date": datetime(2021, 12, 1),
    "catchup": False,
    "on_failure_callback": task_fail_slack_alert,
    "on_success_callback": task_succeed_slack_alert
}


################################## DAG ######################################
@dag(
    schedule_interval='@daily',
    default_args=default_args,
    start_date=datetime(2022, 1, 1),
    catchup=False,
    tags=['airbyte']
)
def airbyte_dag():
    '''
    ### Airbyte DAG
    This is an ELT data pipeline dag that makes POST requests to Airbyte's API.
    The operator will kick off an airbyte sync job, after which the DBT operation will run the transformation piece
    of the Airflow dag.
    '''

    # Dummy operator: usually used as a start node
    t0 = DummyOperator(
        task_id='start'
    )

    extract_olap = AirbyteTriggerSyncOperator(
        task_id='extract_olap',
        airbyte_conn_id='airbyte',
        connection_id=connection_id,
        asynchronous=False,
        timeout=3600,
        wait_seconds=3
    )

    dbt_run = DbtRunOperator(
        task_id="dbt_run",
        dir="/usr/local/airflow/dags/dbt/",  # sample dir
        dbt_bin='/home/astro/.local/bin/dbt',
        profiles_dir='/usr/local/airflow/dags/dbt/',
        trigger_rule="all_done",  # Run even if previous tasks failed
    )

    # t0 >> create_tables >> dbt_seed >> dbt_snapshot >> dbt_run
    t0 >> extract_olap >> dbt_run


airbyte_dag = airbyte_dag()
